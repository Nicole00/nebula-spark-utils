{
  # Spark relation config
  spark: {
    app: {
      name: Nebula Exchange 2.0
    }

    driver: {
      cores: 1
      maxResultSize: 1G
    }

    executor: {
        memory:1G
    }

    cores:{
      max: 16
    }
  }

  # if the hive is hive-on-spark with derby modeï¼Œ you can ignore this hive configure
  # get the config values from file $HIVE_HOME/conf/hive-site.xml or hive-default.xml
    hive: {
      waredir: "hdfs://CM-149:9000/user/vesoft/hive/warehouse/"
      connectionURL: "jdbc:mysql://192.168.8.149:3306/metastore?createDatabaseIfNotExist=TRUE"
      connectionDriverName: "com.mysql.cj.jdbc.Driver"
      connectionUserName: "nebula"
      connectionPassword: "Nebula@123"
    }

  # Nebula Graph relation config
  nebula: {
    address:{
      graph:["127.0.0.1:3699"]
      meta:["127.0.0.1:45500"]
    }
    user: user
    pswd: password
    space: test

    # parameters for SST import, not required
    path:{
        local:"/tmp"
        remote:"/sst"
        hdfs.namenode: "hdfs://name_node:9000"
    }

    connection {
      timeout: 3000
      retry: 3
    }

    execution {
      retry: 3
    }

    error: {
      max: 32
      # failed import job will be recorded in output path
      output: /tmp/errors
    }

    rate: {
      limit: 1024
      timeout: 1000
    }
  }

  # Processing tags
  # There are tag config examples for different dataSources.
  tags: [
    # HDFS csv
    # Import mode is sst, just change type.sink to client if you want to use client import mode.
    {
      name: tag-name-1
      type {
        source: csv
        sink: sst
      }
      path: "hdfs://CM-149:9000/data/"
      # if your csv file has no header, then use _c0,_c1,_c2,.. to indicate fields
      fields: [csv-field-0, csv-field-1, csv-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      vertex: {
        field:csv-field-0
      }
      separator: ","
      header: true
      batch: 256
      partition: 32
    }

    # Hive
    {
      name: tag-name-2
      type: {
        source: hive
        sink: client
      }
      exec: "select hive-field0, hive-field1, hive-field2 from database.table"
      fields: [hive-field-0, hive-field-1, hive-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      vertex: {
        field: hive-field-0
        # policy: "hash"
      }
      batch: 256
      partition: 32
    }
  ]

  # Processing edges
  # There are edge config examples for different dataSources.
  edges: [


    # HDFS csv
    {
      name: edge-name-1
      type: {
        source: csv
        sink: client
      }
      path: hdfs edge path 0
      fields: [csv-field-0, csv-field-1, csv-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      source: {
        field: csv-field-0
        #policy: hash
      }
      target: {
        field: csv-field-1
      }
      ranking: csv-field-2
      separator: ","
      header: true
      batch: 256
      partition: 32
    }

    # Hive
    {
      name: edge-name-2
      type: {
        source: hive
        sink: client
      }
      exec: "select hive-field0, hive-field1, hive-field2 from database.table"
      fields: [ hive-field-0, hive-field-1, hive-field-2]
      nebula.fields: [nebula-field-0, nebula-field-1, nebula-field-2]
      source: hive-field-0
      target: hive-field-1
      batch: 256
      partition: 32
    }
  ]
}
